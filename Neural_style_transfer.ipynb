{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad16358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document=imagenet-vgg-verydeep-19.mat 'https://storage.googleapis.com/marketing-files/colab-notebooks/style-transfer/imagenet-vgg-verydeep-19.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    IMAGE_WIDTH = 400\n",
    "    IMAGE_HEIGHT = 300\n",
    "    COLOR_CHANNELS = 3\n",
    "    NOISE_RATIO = 0.6\n",
    "    MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) \n",
    "    VGG_MODEL = 'imagenet-vgg-verydeep-19.mat' # Pick the VGG 19-layer model by from the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "    STYLE_IMAGE = 'images/stone_style.jpg' # Style image to use.\n",
    "    CONTENT_IMAGE = 'images/content300.jpg' # Content image to use.\n",
    "    OUTPUT_DIR = 'output/'\n",
    "    \n",
    "def load_vgg_model(path):\n",
    "    \"\"\"\n",
    "    Returns a model for the purpose of 'painting' the picture.\n",
    "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
    "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
    "    the paper indicates that using AveragePooling yields better results.\n",
    "    The last few fully connected layers are not used.\n",
    "    Here is the detailed configuration of the VGG model:\n",
    "        0 is conv1_1 (3, 3, 3, 64)\n",
    "        1 is relu\n",
    "        2 is conv1_2 (3, 3, 64, 64)\n",
    "        3 is relu    \n",
    "        4 is maxpool\n",
    "        5 is conv2_1 (3, 3, 64, 128)\n",
    "        6 is relu\n",
    "        7 is conv2_2 (3, 3, 128, 128)\n",
    "        8 is relu\n",
    "        9 is maxpool\n",
    "        10 is conv3_1 (3, 3, 128, 256)\n",
    "        11 is relu\n",
    "        12 is conv3_2 (3, 3, 256, 256)\n",
    "        13 is relu\n",
    "        14 is conv3_3 (3, 3, 256, 256)\n",
    "        15 is relu\n",
    "        16 is conv3_4 (3, 3, 256, 256)\n",
    "        17 is relu\n",
    "        18 is maxpool\n",
    "        19 is conv4_1 (3, 3, 256, 512)\n",
    "        20 is relu\n",
    "        21 is conv4_2 (3, 3, 512, 512)\n",
    "        22 is relu\n",
    "        23 is conv4_3 (3, 3, 512, 512)\n",
    "        24 is relu\n",
    "        25 is conv4_4 (3, 3, 512, 512)\n",
    "        26 is relu\n",
    "        27 is maxpool\n",
    "        28 is conv5_1 (3, 3, 512, 512)\n",
    "        29 is relu\n",
    "        30 is conv5_2 (3, 3, 512, 512)\n",
    "        31 is relu\n",
    "        32 is conv5_3 (3, 3, 512, 512)\n",
    "        33 is relu\n",
    "        34 is conv5_4 (3, 3, 512, 512)\n",
    "        35 is relu\n",
    "        36 is maxpool\n",
    "        37 is fullyconnected (7, 7, 512, 4096)\n",
    "        38 is relu\n",
    "        39 is fullyconnected (1, 1, 4096, 4096)\n",
    "        40 is relu\n",
    "        41 is fullyconnected (1, 1, 4096, 1000)\n",
    "        42 is softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    vgg = scipy.io.loadmat(path)\n",
    "\n",
    "    vgg_layers = vgg['layers']\n",
    "    \n",
    "    def _weights(layer, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and bias from the VGG model for a given layer.\n",
    "        \"\"\"\n",
    "        wb = vgg_layers[0][layer][0][0][2]\n",
    "        W = wb[0][0]\n",
    "        b = wb[0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer):\n",
    "        \"\"\"\n",
    "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
    "        Conv2d layer input.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(prev_layer, filters=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D + RELU layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        \"\"\"\n",
    "        Return the AveragePooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph model.\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def generate_noise_image(content_image, noise_ratio = CONFIG.NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Generates a noisy image by adding random noise to the content_image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a random noise_image\n",
    "    noise_image = np.random.uniform(-20, 20, (1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype('float32')\n",
    "    \n",
    "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    \n",
    "    return input_image\n",
    "\n",
    "\n",
    "def reshape_and_normalize_image(image):\n",
    "    \"\"\"\n",
    "    Reshape and normalize the input image (content or style)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape image to mach expected input of VGG16\n",
    "    image = cv2.resize(image,(400,300))\n",
    "    image = np.reshape(image, ((1,) + image.shape))\n",
    "    \n",
    "    # Substract the mean to match the expected input of VGG16\n",
    "    image = image - CONFIG.MEANS\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def save_image(path, image):\n",
    "    \n",
    "    # Un-normalize the image so that it looks good\n",
    "    image = image + CONFIG.MEANS\n",
    "    \n",
    "    # Clip and Save the image\n",
    "    image = np.clip(image[0], 0, 255).astype('uint8')\n",
    "    imageio.imwrite(path, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = imageio.imread(\"images/louvre.jpg\")\n",
    "imshow(content_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa48b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    \n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    m, n_H, n_W, n_C = a_G.shape\n",
    "    \n",
    "    new_shape = [int(m), int(n_H * n_W), int(n_C)]\n",
    "    \n",
    "    # Reshape a_C and a_G (≈2 lines)\n",
    "    a_C_unrolled = tf.reshape(a_C, new_shape)\n",
    "    a_G_unrolled = tf.reshape(a_G, new_shape)\n",
    "    \n",
    "    # compute the cost with tensorflow (≈1 line)\n",
    "    J_content = (.25 / float(int(n_H * n_W * n_C))) * tf.reduce_sum(tf.math.pow(a_G_unrolled - a_C_unrolled, 2))\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as test:\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "    a_C = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    J_content = compute_content_cost(a_C, a_G)\n",
    "    print(\"J_content = \" + str(J_content.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = imageio.imread(\"images/sandstone.jpg\")\n",
    "imshow(style_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    GA = tf.matmul(A, tf.transpose(A))\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dacd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as test:\n",
    "    tf.random.set_seed(1)\n",
    "    A = tf.random.normal([3, 2*1], mean=1, stddev=4)\n",
    "    GA = gram_matrix(A)\n",
    "    \n",
    "    print(\"GA = \\n\" + str(GA.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    \n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)\n",
    "    a_S = tf.reshape(a_S, [n_H * n_W, n_C])\n",
    "    a_S = tf.transpose(a_S)\n",
    "    a_G = tf.reshape(a_G, [n_H * n_W, n_C])\n",
    "    a_G = tf.transpose(a_G)\n",
    "\n",
    "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    # Computing the loss (≈1 line)\n",
    "    factor = (.5 / (n_H * n_W * n_C)) ** 2\n",
    "    J_style_layer = factor * tf.reduce_sum(tf.math.pow(GS - GG, 2))\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as test:\n",
    "    tf.random.set_seed(1)\n",
    "    a_S = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
    "    \n",
    "    print(\"J_style_layer = \" + str(J_style_layer.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5db69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('conv1_1', 0.2),\n",
    "    ('conv2_1', 0.2),\n",
    "    ('conv3_1', 0.2),\n",
    "    ('conv4_1', 0.2),\n",
    "    ('conv5_1', 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4df6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(model, STYLE_LAYERS):\n",
    "    # initialize the overall style cost\n",
    "    J_style = 0\n",
    "\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "\n",
    "        # Select the output tensor of the currently selected layer\n",
    "        out = model[layer_name]\n",
    "\n",
    "        # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out\n",
    "        a_S = sess.run(out)\n",
    "\n",
    "        # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] \n",
    "        # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "        # when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "        a_G = out\n",
    "        \n",
    "        # Compute style_cost for the current layer\n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
    "\n",
    "        # Add coeff * J_style_layer of this layer to overall style cost\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    J = alpha * J_content + beta * J_style    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as test:\n",
    "    np.random.seed(3)\n",
    "    J_content = np.random.randn()    \n",
    "    J_style = np.random.randn()\n",
    "    J = total_cost(J_content, J_style)\n",
    "    print(\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Start interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = imageio.imread(\"images/louvre.jpg\")\n",
    "\n",
    "content_image = reshape_and_normalize_image(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = imageio.imread(\"images/style300.jpg\")\n",
    "style_image = reshape_and_normalize_image(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fe8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generate_noise_image(content_image)\n",
    "imshow(generated_image[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906199fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0240cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the content image to be the input of the VGG model.  \n",
    "sess.run(model['input'].assign(content_image))\n",
    "\n",
    "# Select the output tensor of layer conv4_2\n",
    "out = model['conv5_2']\n",
    "\n",
    "# Set a_C to be the hidden layer activation from the layer we have selected\n",
    "a_C = sess.run(out)\n",
    "\n",
    "# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n",
    "# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "a_G = out\n",
    "\n",
    "# Compute the content cost\n",
    "J_content = compute_content_cost(a_C, a_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af21972",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(model['input'].assign(style_image))\n",
    "\n",
    "# Compute the style cost\n",
    "J_style = compute_style_cost(model, STYLE_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = total_cost(J_content, J_style, alpha = 10, beta = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(2.0)\n",
    "\n",
    "# define train_step (1 line)\n",
    "train_step = optimizer.minimize(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8df089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nn(sess, input_image, num_iterations = 200):\n",
    "    \n",
    "    # Initialize global variables (you need to run the session on the initializer)\n",
    "    \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Run the noisy input image (initial generated image) through the model. Use assign().\n",
    "   \n",
    "    sess.run(model['input'].assign(input_image))\n",
    "    \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "    \n",
    "        # Run the session on the train_step to minimize the total cost\n",
    "       \n",
    "        somthing = sess.run(train_step)\n",
    "        \n",
    "        \n",
    "        # Compute the generated image by running the session on the current model['input']\n",
    "        \n",
    "        generated_image = sess.run(model['input'])\n",
    "\n",
    "        # Print every 20 iteration.\n",
    "        if i%10 == 0:\n",
    "            Jt, Jc, Js = sess.run([J, J_content, J_style])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"total cost = \" + str(Jt))\n",
    "            print(\"content cost = \" + str(Jc))\n",
    "            print(\"style cost = \" + str(Js))\n",
    "            \n",
    "            # save current generated image in the \"/output\" directory\n",
    "            save_image(\"output/\" + str(i) + \".png\", generated_image)\n",
    "    \n",
    "    # save last generated image\n",
    "    save_image('output/generated_image.jpg', generated_image)\n",
    "    \n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426eac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn(sess, generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cfdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
